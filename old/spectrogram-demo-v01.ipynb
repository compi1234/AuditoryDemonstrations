{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6uLS2mjFAGy"
   },
   "source": [
    "# Spectrogram Plotting\n",
    "This example script plots a waveform / spectrogram and a corresponding segmentation\n",
    "\n",
    "Note: this example notebook does the plotting directly in matplotlib and has a minimum of other dependencies\n",
    "+ librosa is used for the spectrogram computations\n",
    "+ soundfile is used for importing audiofiles\n",
    "+ examples are given for importing data from url resources\n",
    "+ for compliance testing between librosa, Kaldi, SPRAAK and torchaudio see spectrogram-librosa-torch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nky_0yB5kLx_"
   },
   "source": [
    "## First do all the imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1408,
     "status": "ok",
     "timestamp": 1605602917487,
     "user": {
      "displayName": "Dirk Van Compernolle",
      "photoUrl": "",
      "userId": "01011017094742215648"
     },
     "user_tz": -60
    },
    "id": "-g495yAlFAG1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os,sys,io \n",
    "import scipy.signal\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import display, Audio, HTML\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "import librosa as librosa\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "try:\n",
    "  import soundfile as sf\n",
    "except:\n",
    "  ! pip -q install soundfile\n",
    "  import soundfile as sf\n",
    "    \n",
    "from spectrogram_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoiV7JEf6OkL"
   },
   "source": [
    "## Import data\n",
    "- sampled data, reads\n",
    "  + wavdata  (numpy array)\n",
    "  + sr       (int)\n",
    "- different levels of segmentation  In the example code \n",
    "  + TIMIT style segmentations are being read \n",
    "  + sample based segmentations are converted time based segmentations\n",
    "  + segmentations are stored in a dataframe with columns ['t0','t1','seg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1397,
     "status": "ok",
     "timestamp": 1605604472382,
     "user": {
      "displayName": "Dirk Van Compernolle",
      "photoUrl": "",
      "userId": "01011017094742215648"
     },
     "user_tz": -60
    },
    "id": "ZNZ-uu27NqhB"
   },
   "outputs": [],
   "source": [
    "dir='https://raw.githubusercontent.com/compi1234/spchlab/master/data/'\n",
    "wavfile = \"timit/si1027.wav\" #@param {type:\"string\"}\n",
    "sent_segmentation = \"timit/si1027.txt\" #@param {type:\"string\"}\n",
    "word_segmentation = \"timit/si1027.wrd\" #@param {type:\"string\"}\n",
    "phone_segmentation = \"timit/si1027.phn\" #@param {type:\"string\"}\n",
    "word_segmentation = \"timit/si1027.wrd\" #@param {type:\"string\"}\n",
    "phone_mapping = \"timit/phones-61-48-39.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# read the datafiles\n",
    "wavdata, sr = read_audio_from_url(dir+wavfile)\n",
    "transcript = io.StringIO(urlopen(dir+sent_segmentation).read().decode('utf-8')).readline().strip().split(None,2)[2]\n",
    "segwrd = pd.read_csv(dir+word_segmentation,delim_whitespace=True,names=['t0','t1','seg'])\n",
    "segphn = pd.read_csv(dir+phone_segmentation,delim_whitespace=True,names=['t0','t1','seg'])\n",
    "\n",
    "# convert sample based segmentations to time based segmentations\n",
    "segphn['t0']=indx2t(segphn['t0'],sr)\n",
    "segphn['t1']=indx2t(segphn['t1'],sr)\n",
    "segwrd['t0']=indx2t(segwrd['t0'],sr)\n",
    "segwrd['t1']=indx2t(segwrd['t1'],sr)\n",
    "\n",
    "# convert TIMIT61 to TIMIT39\n",
    "# this routine is not great as it simply maps closures to silence instead to combining\n",
    "phone_tbl = pd.read_csv(dir+phone_mapping,delim_whitespace=True,names=['T61','T48','T39'])\n",
    "map61_39 = dict(zip(phone_tbl['T61'],phone_tbl['T39']))\n",
    "segphn39 = segphn.replace({\"seg\": map61_39})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeS_IaBe763j"
   },
   "source": [
    "## Compute Spectrograms and Plot them\n",
    "#### Basic Waveform + Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "executionInfo": {
     "elapsed": 1868,
     "status": "ok",
     "timestamp": 1605605398062,
     "user": {
      "displayName": "Dirk Van Compernolle",
      "photoUrl": "",
      "userId": "01011017094742215648"
     },
     "user_tz": -60
    },
    "id": "idLIuZDUfCHb",
    "outputId": "8cf8be97-1b1b-4d50-da64-c67797faaf9e"
   },
   "outputs": [],
   "source": [
    "spg = spectrogram(wavdata,samplerate=sr)\n",
    "fig=plot_spg(spg=[spg], wav=wavdata)\n",
    "ax = fig.get_axes()\n",
    "ax[1].set_xlabel('Frame')\n",
    "ax[1].set_ylabel('Frequency Coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Spectrograms and Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAc4nKdh5fVz"
   },
   "outputs": [],
   "source": [
    "# compute a standard spectrogram and a 80 and 24 channel mel filterbank\n",
    "spg = spectrogram(wavdata,samplerate=sr,n_mels=None)\n",
    "spg80 = spectrogram(wavdata,samplerate=sr,n_mels=80)\n",
    "spg24 = spectrogram(wavdata,samplerate=sr,n_mels=24)\n",
    "# display audio button and the spectrograms ; add additional lables\n",
    "display(Audio(data=wavdata,rate=sr))\n",
    "fig=plot_spg(spg=[spg,spg80,spg24], wav=wavdata, seg=[segphn,segwrd],txt=transcript,figsize=(12,8),spg_scale=1.5)\n",
    "ax = fig.get_axes()\n",
    "ax[1].set_ylabel('frequency')\n",
    "ax[2].set_ylabel('mel fbank(80)')\n",
    "ax[3].set_ylabel('mel fbank(24)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "melspec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
